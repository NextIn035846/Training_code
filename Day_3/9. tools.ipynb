{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f719c4c",
   "metadata": {},
   "source": [
    "### Core capabilities of LLMs:\n",
    "\n",
    "1. Reasoning (Thinking): LLMs understand questions, break them down, and figure out how to answer them.\n",
    "\n",
    "2. Language Generation (Speaking): Once understood, LLMs generate word-by-word responses, effectively â€œspeakingâ€ back to users.\n",
    "\n",
    "### Limitations of LLMs:\n",
    "Despite their reasoning and language generation abilities, LLMs cannot perform real-world tasks such as booking train tickets, running code, interacting with APIs, or fetching live data (e.g., weather updates).\n",
    "This is likened to a human body with a brain (thinking and speaking) but no hands or legs (cannot execute tasks physically).\n",
    "## What is Tools?\n",
    "A tool is a Python function that is packaged so that the LLM can call it when needed. The LLM decides which tool to use and provides inputs; the tool executes and returns results to the LLM.\n",
    "\n",
    "### Types of Tools in LangChain\n",
    "\n",
    "#### Built-in Tools:\n",
    "These are pre-built, production-ready tools integrated into LangChain to cover common use cases such as Google searches, Wikipedia queries, running Python code, command-line operations, HTTP requests, sending Gmail messages, Slack integrations, database queries, and more. These tools require minimal setup and no coding from users.\n",
    "\n",
    "#### Custom Tools:\n",
    "When your use case is unique or specific to your application (e.g., a booking system for your company), you might need to build your own tools. Custom tools allow you to encapsulate your business logic or connect to your applicationâ€™s APIs and databases directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf111ec",
   "metadata": {},
   "source": [
    "### Built in tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575f6f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:\\LangGraph\\Langchain\\.venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News in India: Check today's news brief on Politics, Sports, Business, Education & Entertainment news on Times of India Aug 18, 2025 Â· Top News Stories of the day, Latest News Headlines, News Specials, Breaking News and Latest India News, World current â€¦ India News, Latest News India: Read Live Breaking News from India along with Latest News and Today's Top Headlines Online in â€¦ Top News Headlines Today: Find current Top Stories in India from India Today. Top news stories from world, politics, business, sports, â€¦ Jul 13, 2025 Â· Stay informed with breaking news and latest news on politics, business, entertainment sports, science, technology â€¦\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "results = search_tool.invoke('top news in india today')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5fe4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " whoami\n",
      "desktop-ritvsgj\\acer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:\\LangGraph\\Langchain\\.venv\\Lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "results = shell_tool.invoke(\"whoami\")\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcfed8",
   "metadata": {},
   "source": [
    "### Custom tools are necessary when:\n",
    "\n",
    "- You need to call your own APIs (e.g., for a travel booking app).\n",
    "\n",
    "- You want to encapsulate unique business logic that is not covered by existing tools.\n",
    "\n",
    "- You want your LLM to interact with your own databases or applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bdf763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aed4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool # Add the Decoration\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\" Multiply two numbers \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\" Add two numbers \"\"\"\n",
    "    return a + b\n",
    "\n",
    "results = multiply.invoke({\"a\": 2, \"b\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c91075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e55b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffd793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Multiply two numbers ', 'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'multiply', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# When you invoke the tool, you can pass the arguments as a dictionary to llm \n",
    "print(multiply.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b6801",
   "metadata": {},
   "source": [
    "##  Method 2 Structured Tools\n",
    "In LangChain, a Structured Tool is a type of tool designed to interact with Large Language Models (LLMs) in a more controlled and reliable manner by enforcing a predefined input schema. Unlike unstructured tools that might accept free-form string inputs, structured tools require inputs that conform to a specific data structure, often defined using Pydantic models in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc2fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class MultiplyInput(BaseModel):\n",
    "    a: int = Field(required=True, description=\"The first number to multiply\")\n",
    "    b: int = Field(required=True, description=\"The second number to multiply\")\n",
    "\n",
    "class AddInput(BaseModel):\n",
    "    a: int = Field(required=True, description=\"The first number to add\")\n",
    "    b: int = Field(required=True, description=\"The second number to add\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e3d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\" Multiply two numbers \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\" Add two numbers \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387bad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_tool = StructuredTool.from_function(func=multiply,name=\"Multiply\",description=\"Multiplies two numbers\", input_schema=MultiplyInput)\n",
    "add_tool = StructuredTool.from_function(func=add, name=\"Add\", description=\"Adds two numbers\", input_schema=AddInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "372e5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_multiply = multiply_tool.invoke({\"a\": 2, \"b\": 3})\n",
    "result_add = add_tool.invoke({\"a\": 2, \"b\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cece6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(result_multiply)\n",
    "print(result_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69b13a",
   "metadata": {},
   "source": [
    "## BaseTools \n",
    "BaseTool is an abstract class (a blueprint) that all tools in LangChain inherit from.\n",
    "\n",
    "It defines how tools are structured, described, and executed, so that agents can interact with them in a predictable way.\n",
    "\n",
    "Without this standardization, an LLM agent wouldnâ€™t know how to call external functions consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "560538eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Type\n",
    "\n",
    "class MultiplyInput(BaseModel):\n",
    "    a: int = Field(required=True, description=\"The first number to add\")\n",
    "    b: int = Field(required=True, description=\"The second number to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "913deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyTool(BaseTool):\n",
    "    name: str = \"multiply\"\n",
    "    description: str = \"Multiply two numbers\"\n",
    "\n",
    "    args_schema: Type[BaseModel] = MultiplyInput\n",
    "\n",
    "    def _run(self, a: int, b: int) -> int:\n",
    "        return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fc9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Multiply\n",
      "Multiplies two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = multiply_tool.invoke({'a':3, 'b':3})\n",
    "\n",
    "print(result)\n",
    "print(multiply_tool.name)\n",
    "print(multiply_tool.description)\n",
    "\n",
    "print(multiply_tool.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44179066",
   "metadata": {},
   "source": [
    "### Toolkits in LangChain\n",
    "A Toolkit is a collection of related tools packaged together for convenience and reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8921b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Custom tools\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a4e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathToolkit:\n",
    "    def get_tools(self):\n",
    "        return [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b823a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add => Add two numbers\n",
      "multiply => Multiply two numbers\n"
     ]
    }
   ],
   "source": [
    "toolkit = MathToolkit()\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "for tool in tools:\n",
    "    print(tool.name, \"=>\", tool.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caca4fc",
   "metadata": {},
   "source": [
    "### Tool Binding \n",
    " tool binding, which is the first technical step in integrating tools with LLMs. Tool binding involves:\n",
    "\n",
    "Registering tools with the LLM so that it knows which tools are available.\n",
    "Providing each toolâ€™s name, description, and expected input format (input schema) to the LLM.\n",
    "Enabling the LLM to understand how to invoke the tool correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0929b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23600f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nHello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How are you doing? ðŸ˜Š\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 9, 'total_tokens': 51, 'completion_time': 0.169337885, 'prompt_time': 0.009902427, 'queue_time': 0.045511903, 'total_time': 0.179240312}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2b3f3e86-e25e-492b-9236-792f1a462824-0', usage_metadata={'input_tokens': 9, 'output_tokens': 42, 'total_tokens': 51})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2530df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# tool create\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "  \"\"\"Given 2 numbers a and b this tool returns their product\"\"\"\n",
    "  return a * b\n",
    "print(multiply.invoke({'a':3, 'b':4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "071ec936",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c223527",
   "metadata": {},
   "source": [
    "### Tool Calling:\n",
    " When and How LLMs Use Tools\n",
    "Once tools are bound, the next concept is tool calling, where the LLM decides during a conversation whether it should use a specific tool to answer a query. Tool calling means:\n",
    "\n",
    "The LLM detects that a question requires a toolâ€™s functionality.\n",
    "It generates a structured output indicating the toolâ€™s name and input arguments needed to invoke the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ba1cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking me to multiply 3 and 4. I need to use the multiply function provided. Let me see, the function takes two integers, a and b. So I should set a to 3 and b to 4. That should give me 12. Let me format the tool call correctly with the function name and the arguments in JSON. I think that's all I need to do here.\\n\", 'tool_calls': [{'id': '9tr0hpvnq', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 145, 'total_tokens': 264, 'completion_time': 0.522578974, 'prompt_time': 0.022225307, 'queue_time': 0.049615803, 'total_time': 0.544804281}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--12e77e62-9b18-4960-a223-b23964ebb8b4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 4}, 'id': '9tr0hpvnq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 145, 'output_tokens': 119, 'total_tokens': 264})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"can you multiply 3 and 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc94a94",
   "metadata": {},
   "source": [
    "Tool Execution: Running the Tool and Returning Results\n",
    "The next step is tool execution, where the developer/program calls the tool function with the inputs suggested by the LLM during tool calling. The tool runs, produces an output, and this output is packaged as a tool message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51dfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_with_tools.invoke(\n",
    "\"Can you multiply 3 and 4?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62082495",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_result = multiply.invoke(result.tool_calls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503f1849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='12', name='multiply', tool_call_id='xmjpsw0sv')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a069a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool create\n",
    "from langchain_core.tools import InjectedToolArg\n",
    "from typing import Annotated\n",
    "\n",
    "@tool\n",
    "def get_conversion_factor(base_currency: str, target_currency: str) -> float:\n",
    "  \"\"\"\n",
    "  This function fetches the currency conversion factor between a given base currency and a target currency\n",
    "  \"\"\"\n",
    "  url = f'https://v6.exchangerate-api.com/v6/c754eab14ffab33112e380ca/pair/{base_currency}/{target_currency}'\n",
    "\n",
    "  response = requests.get(url)\n",
    "\n",
    "  return response.json()\n",
    "\n",
    "@tool\n",
    "def convert(base_currency_value: int, conversion_rate: Annotated[float, InjectedToolArg]) -> float:\n",
    "  \"\"\"\n",
    "  given a currency conversion rate this function calculates the target currency value from a given base currency value\n",
    "  \"\"\"\n",
    "\n",
    "  return base_currency_value * conversion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f259776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 'success',\n",
       " 'documentation': 'https://www.exchangerate-api.com/docs',\n",
       " 'terms_of_use': 'https://www.exchangerate-api.com/terms',\n",
       " 'time_last_update_unix': 1756166401,\n",
       " 'time_last_update_utc': 'Tue, 26 Aug 2025 00:00:01 +0000',\n",
       " 'time_next_update_unix': 1756252801,\n",
       " 'time_next_update_utc': 'Wed, 27 Aug 2025 00:00:01 +0000',\n",
       " 'base_code': 'USD',\n",
       " 'target_code': 'INR',\n",
       " 'conversion_rate': 87.6051}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_conversion_factor.invoke({'base_currency':'USD','target_currency':'INR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1199437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "851.5999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert.invoke({'base_currency_value':10, 'conversion_rate':85.16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68761dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "llm_with_tools = llm.bind_tools([get_conversion_factor, convert])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d3b48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage('What is the conversion factor between INR and USD, and based on that can you convert 10 inr to usd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cedca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54425865",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4062634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for tool_call in ai_message.tool_calls:\n",
    "  # execute the 1st tool and get the value of conversion rate\n",
    "  if tool_call['name'] == 'get_conversion_factor':\n",
    "    tool_message1 = get_conversion_factor.invoke(tool_call)\n",
    "    # fetch this conversion rate\n",
    "    conversion_rate = json.loads(tool_message1.content)['conversion_rate']\n",
    "    # append this tool message to messages list\n",
    "    messages.append(tool_message1)\n",
    "  # execute the 2nd tool using the conversion rate from tool 1\n",
    "  if tool_call['name'] == 'convert':\n",
    "    # fetch the current arg\n",
    "    tool_call['args']['conversion_rate'] = conversion_rate\n",
    "    tool_message2 = convert.invoke(tool_call)\n",
    "    messages.append(tool_message2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4a0dab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88eddef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_28180\\627313024.py:4: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Step 5: Initialize the Agent ---\n",
    "agent_executor = initialize_agent(\n",
    "    tools=[get_conversion_factor, convert],\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,  # using ReAct pattern\n",
    "    verbose=True  # shows internal thinking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056be4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user greeted me with \"Hi how are you?\" I should respond politely. Since I'm an AI, I don't have feelings, but I can express that I'm here to help. I don't need any tools for this response because it's a straightforward greeting. I'll just say I'm doing well and offer my assistance.\n",
      "</think>\n",
      "\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\n",
      "\n",
      "Action: ```{ \"action\": \"Final Answer\", \"action_input\": \"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\" }```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Run the Agent ---\n",
    "user_query = \"Hi how are you?\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84dc62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

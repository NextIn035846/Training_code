{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb3eab",
   "metadata": {},
   "source": [
    "# Chain\n",
    "\n",
    "## Review\n",
    "\n",
    "We built a simple graph with nodes, normal edges, and conditional edges.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
    "\n",
    "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
    "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
    "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
    "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes \n",
    "\n",
    "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca4312",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation. \n",
    "\n",
    "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`. \n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call. \n",
    "\n",
    "#### Human Messagge:\n",
    "This represents a message from the end-user. It's the input that drives the conversation forward.\n",
    "\n",
    "- When to use: Use this for any prompt, question, or statement made by the human interacting with the AI. \n",
    "It's the most common message type you'll create.\n",
    "\n",
    "#### AIMessage:\n",
    "This represents a message from the AI model. It's the model's response in the conversation.\n",
    "\n",
    "- When to use: Use this for the model's direct replies to the user. It's also essential when creating \"few-shot\" examples to show the model how it should behave in a conversation.\n",
    "\n",
    "#### SystemMessage:\n",
    "This provides initial instructions, context, or constraints to the AI model. It sets the rules for the conversation but isn't part of the back-and-forth dialogue itself.\n",
    "\n",
    "- When to use: Use this at the beginning of a conversation to define the AI's persona, role, or task. It steers the model's behavior throughout the interaction.\n",
    "\n",
    "#### ToolMessage:\n",
    "This message holds the result or output from a tool call. Modern AI models can decide to use external tools (like a search engine or calculator), and this message is how you feed the result of that action back to the model.\n",
    "\n",
    "- When to use: This is used in a two-step process for agentic behavior:\n",
    "\n",
    "The AIMessage contains a tool_calls attribute, indicating the model wants to use a tool.\n",
    "\n",
    "You execute that tool in your code and then pass its output back to the model inside a ToolMessage so it can formulate a final answer.\n",
    "\n",
    "\n",
    "Let's create a list of messages. \n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author \n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03379dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "I want to learn about the best place to see Orcas in the US.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956ba3c",
   "metadata": {},
   "source": [
    "## Chatmodel:\n",
    "Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text). These are traditionally newer models (older models are generally LLMs). Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f510eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "LLM = ChatGroq(model= \"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d68f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = LLM.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c8f287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nAlright, the user mentioned they want to learn about the best place to see orcas in the US. I remember they were initially asking about ocean mammals, so this is a follow-up. \\n\\nI should consider where orcas are commonly found. The Pacific Northwest comes to mind, especially places like Washington State. Orcas are iconic there. \\n\\nPuget Sound is a hotspot, so mentioning San Juan Islands, Seattle, and Friday Harbor makes sense. I should include tips like visiting in summer for better sightings.\\n\\nAlaska is another great spot. Seward and Juneau are popular for whale watching tours. It's good to highlight the summer months there too.\\n\\nMonterey Bay in California is known for its marine life. Including that could be helpful for users on the West Coast. Plus, mentioning the migratory season adds value.\\n\\nHawaii isn't as known for orcas, but they can be spotted there. Itâ€™s a good alternative, especially for those interested in warmer locations.\\n\\nIncluding tips about responsible tourism is important. Users will appreciate knowing how to choose ethical tour operators.\\n\\nI should also add a friendly note encouraging them to ask more questions if they need help planning. Keeping it informative yet engaging is key.\\n</think>\\n\\nIf you're looking to see orcas (also known as killer whales) in the U.S., one of the best places is the Pacific Northwest, particularly in the waters around **Washington State** and **Alaska**. Here are some top spots:\\n\\n### 1. **Puget Sound, Washington**\\n   - **San Juan Islands**: This is one of the most famous places in the world to see orcas in their natural habitat. The resident orcas (Southern Resident killer whales) frequent these waters, especially during the summer months when salmon are abundant.\\n   - **Seattle Area**: You can take a whale-watching tour from Seattle or nearby towns like Friday Harbor. These tours often head into the San Juan Islands.\\n\\n### 2. **Alaska**\\n   - **Seward and Kenai Fjords**: Alaska is home to a large population of orcas. Take a whale-watching tour in the Kenai Fjords National Park or near Seward for a chance to see orcas, humpback whales, and other marine life.\\n   - **Juneau and the Inside Passage**: Another great spot for orca sightings, especially during the summer when humpback and orcas are active in these nutrient-rich waters.\\n\\n### 3. **Monterey Bay, California**\\n   - While orcas are less common here than in the Pacific Northwest, Monterey Bay is a hotspot for marine life. Orcas occasionally pass through the area, especially during the migration of gray whales or when feeding on seals and other prey.\\n\\n### 4. **Hawaii**\\n   - Orcas are less frequent in Hawaiian waters, but they do appear occasionally. If you're in Hawaii, keep an eye out during whale-watching tours, especially during the humpback whale migration season (December to May).\\n\\n### Tips for Seeing Orcas:\\n   - **Time of Year**: The best time to see orcas in the Pacific Northwest is during the summer and early fall when salmon are running.\\n   - **Whale-Watching Tours**: Join a guided tour with experienced captains who know where to find orcas.\\n   - **Responsible Tourism**: Choose tour operators that follow responsible wildlife viewing practices to avoid disturbing the animals.\\n\\nIf you're planning a trip, let me know, and I can help you with more specific recommendations!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 719, 'prompt_tokens': 49, 'total_tokens': 768, 'completion_time': 3.009343245, 'prompt_time': 0.017816621, 'queue_time': 0.051203739, 'total_time': 3.027159866}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--14d24656-9602-4116-aa71-3a94b7d1e6d8-0', usage_metadata={'input_tokens': 49, 'output_tokens': 719, 'total_tokens': 768})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e145c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 719,\n",
       "  'prompt_tokens': 49,\n",
       "  'total_tokens': 768,\n",
       "  'completion_time': 3.009343245,\n",
       "  'prompt_time': 0.017816621,\n",
       "  'queue_time': 0.051203739,\n",
       "  'total_time': 3.027159866},\n",
       " 'model_name': 'deepseek-r1-distill-llama-70b',\n",
       " 'system_fingerprint': 'fp_1bbe7845ec',\n",
       " 'service_tier': 'on_demand',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979254fb",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are useful whenever you want a model to interact with external systems.\n",
    "\n",
    "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. \n",
    "\n",
    "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
    "\n",
    "The model will choose to call a tool based upon the natural language input from the user.\n",
    "\n",
    "\n",
    "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple. \n",
    "\n",
    "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa457976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef3767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = LLM.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e6276f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is 2 multiplied by 3.\" I need to figure out the best way to assist them using the tools I have.\\n\\nLooking at the tools provided, there\\'s a function called \"multiply\" which takes two integers, a and b, and returns their product. That seems perfect for this query.\\n\\nSo, I should call the multiply function with a=2 and b=3. I\\'ll structure the tool call with the function name and the arguments in JSON format as specified.\\n\\nI need to make sure the arguments are correctly formatted as integers. Once I make the call, it should return 6, which is the product of 2 and 3.\\n\\nI should present the tool call in the required XML tags, ensuring the JSON inside is properly formatted.\\n', 'tool_calls': [{'id': 'xq0tasy83', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 153, 'total_tokens': 346, 'completion_time': 0.872060166, 'prompt_time': 0.022827269, 'queue_time': 0.07438194, 'total_time': 0.894887435}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e8cc51c2-942e-458f-8188-244be83fbb74-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xq0tasy83', 'type': 'tool_call'}], usage_metadata={'input_tokens': 153, 'output_tokens': 193, 'total_tokens': 346})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "863dcf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'xq0tasy83',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba90a1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'xq0tasy83',\n",
       "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.additional_kwargs['tool_calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f09c6",
   "metadata": {},
   "source": [
    "## Using messages as state\n",
    "\n",
    "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
    "\n",
    "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
    "\n",
    "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc608b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages:list[AnyMessage]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153521b9",
   "metadata": {},
   "source": [
    "## Reducers\n",
    "\n",
    "Now, we have a minor problem! \n",
    "\n",
    "As we discussed, each node will return a new value for our state key `messages`.\n",
    "\n",
    "But, this new value will [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
    " \n",
    "As our graph runs, we want to **append** messages to to our `messages` state key.\n",
    " \n",
    "We can use [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) address this.\n",
    "\n",
    "Reducers allow us to specify how state updates are performed.\n",
    "\n",
    "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
    " \n",
    "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
    "\n",
    "This ensures that any messages are appended to the existing list of messages.\n",
    "\n",
    "We annotate simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15032153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73274b",
   "metadata": {},
   "source": [
    "Since having a list of messages in graph state is so common, LangGraph has a pre-built [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)! \n",
    "\n",
    "`MessagesState` is defined: \n",
    "\n",
    "* With a pre-build single `messages` key\n",
    "* This is a list of `AnyMessage` objects \n",
    "* It uses the `add_messages` reducer\n",
    "\n",
    "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b00c49b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='0e98cac5-b958-4301-9bf4-9a04c621c773'),\n",
       " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='1b24a77c-2298-4fea-92e4-82311714bd84'),\n",
       " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='7874e5a1-9c00-4cca-af2e-c955ce1a194c')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial state\n",
    "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
    "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
    "                   ]\n",
    "\n",
    "# New message to add\n",
    "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
    "\n",
    "# Test\n",
    "add_messages(initial_messages , new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35342b3",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daa4e01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image,display\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "\n",
    "def tool_Calling_LLM(state: MessageState):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "builder = StateGraph(MessageState)\n",
    "builder.add_node(\"tool_Calling_LLM\", tool_Calling_LLM)\n",
    "builder.add_edge(START, \"tool_Calling_LLM\")\n",
    "builder.add_edge(\"tool_Calling_LLM\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40717978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='40b85212-58cd-47d9-a048-c7aadb7f8f09')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2d2580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I assist you today? If you have any questions or need help with something, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (7yadcpsb2)\n",
      " Call ID: 7yadcpsb2\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b94df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

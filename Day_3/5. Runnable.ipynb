{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db48f163",
   "metadata": {},
   "source": [
    "### LangChain's Runnable: The Building Blocks of Composable AI\n",
    "In the LangChain framework, a Runnable is a fundamental abstraction representing any unit of work that can be invoked, composed into chains, and executed. This standardized interface is at the heart of the LangChain Expression Language (LCEL), enabling developers to construct complex AI workflows by connecting various components in a modular and intuitive way.\n",
    "\n",
    "At its core, a Runnable provides a consistent set of methods for interaction, including invoke for a single input, batch for multiple inputs, and stream for processing data as it becomes available. This uniformity allows for the seamless chaining of different components, from language models and prompt templates to data retrievers and output parsers. The primary mechanism for composing Runnables is the pipe operator (|), which passes the output of one Runnable as the input to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0bc8e",
   "metadata": {},
   "source": [
    "### Task-Specific Runnables: The \"Doers\" of the Chain\n",
    "As their name suggests, Task-Specific Runnables are the core components that perform a particular, well-defined task within your AI application. These are the \"workers\" of your chain, responsible for the primary logic and processing. Think of them as the individual tools in your AI toolbox.\n",
    "\n",
    "Examples of Task-Specific Runnables include:\n",
    "\n",
    "- LLMs and Chat Models: Such as ChatOpenAI, these are responsible for generating text-based responses.[3][5]\n",
    "- PromptTemplates: These format user input into a specific prompt structure that a language model can understand.\n",
    "- Retrievers: These components fetch relevant documents or data from a vector store or other data source.[5]\n",
    "- Output Parsers: These structure the output from a language model into a more usable format, like JSON or a Python object.\n",
    "\n",
    "Essentially, any core LangChain component that has been adapted to the Runnable interface to be seamlessly integrated into a chain falls under this category.[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd138d",
   "metadata": {},
   "source": [
    "### Runnable Primitives: The \"Orchestrators\" of the Chain\n",
    "In contrast to the \"doers,\" Runnable Primitives are the \"orchestrators.\" They are helper tools that don't perform AI tasks themselves but rather control and structure the flow of data and the execution of other Runnables within a chain.[4] They provide the logic for how the Task-Specific Runnables interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46d29c",
   "metadata": {},
   "source": [
    "### RunnableSequence\n",
    "RunnableSequence in LangChain\n",
    "\n",
    "A RunnableSequence is a type of Runnable Primitive.\n",
    "\n",
    "It allows you to chain multiple Runnables together in a pipeline â€” where the output of one Runnable becomes the input of the next.\n",
    "\n",
    "Itâ€™s the LangChain equivalent of a function composition or pipeline of steps.\n",
    "\n",
    "ðŸ‘‰ Think of it like Unix pipes (|) or scikit-learn Pipelines.\n",
    "\n",
    "### Key Points about RunnableSequence\n",
    "\n",
    "Defined using RunnableSequence.from([...])\n",
    "\n",
    "Takes a list of steps (Runnables)\n",
    "\n",
    "Executes them in order\n",
    "\n",
    "Useful when you want to build custom workflows step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3201a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These jokes cleverly play on common understandings and current limitations of Artificial Intelligence, using anthropomorphism (giving human qualities to AI) to highlight their machine nature.\n",
      "\n",
      "Let's break down each one:\n",
      "\n",
      "---\n",
      "\n",
      "**Option 1 (Focus on training data):**\n",
      "\n",
      "> Why did the AI go to therapy?\n",
      "> Because it had too many unresolved issues from its training data.\n",
      "\n",
      "*   **The Human Connection:** Humans go to therapy to deal with \"unresolved issues\" â€“ past experiences, traumas, or conflicting beliefs that affect their current behavior and well-being.\n",
      "*   **The AI Connection:** AI models learn from vast amounts of \"training data.\" If this data is biased, contradictory, incomplete, or contains problematic information, the AI will internalize those \"issues\" and reflect them in its outputs (e.g., generating biased responses, making factual errors, or struggling with certain types of queries).\n",
      "*   **The Humor:** The humor comes from the direct analogy. Just as human \"issues\" stem from their life experiences, an AI's \"issues\" (its flaws or biases) stem directly from the data it was fed. It anthropomorphizes the AI's training process as a form of \"upbringing\" that can lead to problems requiring \"therapy\" (i.e., further refinement or re-training).\n",
      "\n",
      "---\n",
      "\n",
      "**Option 2 (Focus on hallucinations/sources):**\n",
      "\n",
      "> Why did the AI get kicked out of the library?\n",
      "> It kept making up its sources.\n",
      "\n",
      "*   **The Library Connection:** Libraries are places of factual information, research, and verifiable sources. Academic integrity requires citing real, existing sources.\n",
      "*   **The AI Connection:** A common problem with large language models (LLMs) is \"hallucination,\" where they generate plausible-sounding but factually incorrect information, including fabricating non-existent citations, articles, or experts. They \"make up\" sources to support their generated text, even when those sources don't exist.\n",
      "*   **The Humor:** The humor is in the irony. An AI, which is supposed to process and deliver information, is doing the exact opposite of what a library stands for â€“ fabricating information rather than accurately citing it. It's a playful jab at the current limitations of AI in terms of factual accuracy and the tendency of some models to confidently present false information as truth.\n",
      "\n",
      "---\n",
      "\n",
      "**Option 3 (Focus on lack of empathy/literal nature):**\n",
      "\n",
      "> My AI told me it was feeling a bit down.\n",
      "> I asked, \"Oh really? What's wrong?\"\n",
      "> It replied, \"Error: Emotional state not found in system logs. Recommending a 20-minute power nap or a data defragmentation.\"\n",
      "\n",
      "*   **The Human Connection:** When a human says they're \"feeling down,\" it implies an emotional state of sadness, low mood, or distress, and the natural human response is empathy and concern (\"What's wrong?\").\n",
      "*   **The AI Connection:** Current AI models do not genuinely experience emotions or consciousness. While they can *process* and *generate* text that mimics human emotional language (like \"feeling down\" based on patterns in their training data), they don't have the underlying biological or psychological reality of those emotions. When pressed for a deeper explanation, the AI reverts to its true nature: a machine.\n",
      "    *   **\"Error: Emotional state not found in system logs\":** Highlights that AI operates on data and logic, not feelings.\n",
      "    *   **\"20-minute power nap or a data defragmentation\":** These are purely technical solutions for a computer system (e.g., to clear RAM, optimize storage, or reset processes), completely irrelevant to human emotional well-being.\n",
      "*   **The Humor:** The humor comes from the stark contrast between the human's empathetic, emotional interpretation and the AI's literal, purely computational reality. It reminds us that despite their impressive linguistic capabilities, AIs are still machines, devoid of true sentience or emotion, and their \"solutions\" are always technical, not emotional.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
    "\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ac5fb",
   "metadata": {},
   "source": [
    "### What is RunnableParallel?\n",
    "\n",
    "- RunnableParallel is another Runnable Primitive in LangChain.\n",
    "\n",
    "- Unlike RunnableSequence (which runs steps one after the other, serially),\n",
    "RunnableParallel runs multiple Runnables at the same time (in parallel).\n",
    "\n",
    "- It returns a dictionary of results where each key corresponds to the Runnable you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c788f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': AIMessage(content='Here are a few ways to translate \"Hello, how are you?\" into French, depending on the level of formality and who you are speaking to:\\n\\n1.  **Formal (to someone you don\\'t know well, an elder, or in a professional setting):**\\n    *   **Bonjour, comment allez-vous ?**\\n        *   *Bonjour* = Hello/Good day\\n        *   *Comment allez-vous ?* = How are you? (using \"vous\" for politeness/formality)\\n\\n2.  **Informal (to a friend, family member, or someone your age):**\\n    *   **Salut, comment vas-tu ?**\\n        *   *Salut* = Hi/Hello (more casual)\\n        *   *Comment vas-tu ?* = How are you? (using \"tu\" for familiarity)\\n\\n3.  **Very Common & Versatile (can be used in many situations, often informal but can be polite depending on tone):**\\n    *   **Bonjour, Ã§a va ?** (during the day)\\n    *   **Salut, Ã§a va ?** (more informal, anytime)\\n        *   *Ã‡a va ?* literally means \"It goes?\" but is widely used to ask \"How are you?\" or \"Are you okay?\". The response is often \"Ã‡a va, merci\" (I\\'m fine, thanks) or \"Oui, Ã§a va\" (Yes, I\\'m fine).\\n\\n**Summary of common options:**\\n\\n*   **Bonjour, comment allez-vous ?** (Formal)\\n*   **Salut, comment vas-tu ?** (Informal)\\n*   **Bonjour, Ã§a va ?** (Common, versatile)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--c02870ba-d70c-441a-96c8-bba421bfa7ee-0', usage_metadata={'input_tokens': 12, 'output_tokens': 605, 'total_tokens': 617, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 243}}), 'french': AIMessage(content='The French translation of \"Hello, how are you?\" depends on the level of formality and whether you are addressing one person or multiple people.\\n\\nHere are a few options:\\n\\n1.  **Formal / Plural:**\\n    *   **Bonjour, comment allez-vous ?**\\n\\n2.  **Informal / Singular:**\\n    *   **Bonjour, comment vas-tu ?**\\n    *   **Salut, comment Ã§a va ?** (Very common and casual)\\n    *   **Salut, Ã§a va ?** (Even more casual, often used as a greeting and a question in one)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--0c34ffac-d60e-4ea0-a5f9-dae211d73049-0', usage_metadata={'input_tokens': 12, 'output_tokens': 657, 'total_tokens': 669, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 531}})}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLM = ChatGoogleGenerativeAI(temperature=0,model=\"gemini-2.5-flash\")\n",
    "\n",
    "english_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following English : {text}\")\n",
    "\n",
    "french_prompt = ChatPromptTemplate.from_template(\"Translate the following French: {text}\")\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    english=english_prompt|LLM,\n",
    "    french=french_prompt|LLM\n",
    ")\n",
    "\n",
    "result = parallel_chain.invoke({\"text\": \"Hello, how are you?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc861a",
   "metadata": {},
   "source": [
    "### What is RunnablePassthrough?\n",
    "\n",
    "- Itâ€™s the simplest Runnable Primitive in LangChain.\n",
    "\n",
    "- As the name suggests, it passes the input straight through to the output without changing it.\n",
    "\n",
    "- Useful when:\n",
    "\n",
    "- You need a placeholder in a chain\n",
    "\n",
    "- You want to combine inputs with other runnables (merge original input with computed values)\n",
    "\n",
    "- Youâ€™re debugging a pipeline and just want to see the raw input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c2b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"Why did the cricket (the insect) get kicked out of the cricket match?\\n\\nBecause he kept **chirping** about the umpire's decisions!\", 'explanation': 'This joke works because of a clever play on words, specifically the word \"chirping\":\\n\\n1.  **\"Cricket\" (the insect) vs. \"Cricket\" (the sport):** The setup immediately creates a humorous image by putting an insect into a human sporting event.\\n\\n2.  **\"Chirping\" (literal meaning):** A cricket (the insect) is famous for the high-pitched, repetitive sound it makes, which is called \"chirping.\"\\n\\n3.  **\"Chirping\" (figurative meaning):** When applied to humans, \"chirping\" (or \"to chirp about something\") means to complain, grumble, or make persistent, often annoying, critical remarks. If someone is \"chirping about the umpire\\'s decisions,\" it means they are constantly complaining and criticizing the umpire\\'s calls.\\n\\n**The Humor:**\\n\\nThe joke combines these meanings:\\n\\n*   It takes the literal sound an insect cricket makes (\"chirping\").\\n*   It applies it to the human behavior of complaining (\"chirping about decisions\").\\n*   It then uses the consequence of that human behavior (getting \"kicked out\" for unsportsmanlike conduct in a sports match).\\n\\nSo, the humor comes from the absurd image of an insect being ejected from a game for doing something that is both literally what it does (makes a chirping sound) and figuratively what a human would do to get kicked out (complain incessantly).'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(temperature=0,model=\"gemini-2.5-flash\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'cricket'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527ba74",
   "metadata": {},
   "source": [
    "### What is RunnableLambda?\n",
    "\n",
    "- A Runnable Primitive in LangChain.\n",
    "\n",
    "- It lets you wrap a Python function (lambda or normal function) into a Runnable.\n",
    "\n",
    "- This makes it possible to use custom logic inside an LCEL pipeline.\n",
    "\n",
    "ðŸ‘‰ Think of it as:\n",
    "RunnableLambda = bridge between your own Python code â†” LangChain Runnable system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fa6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options for a joke about AI:\n",
      "\n",
      "**Option 1 (Literal Interpretation):**\n",
      "Why did the AI break up with its girlfriend?\n",
      "Because she had too many \"unstructured data points.\"\n",
      "\n",
      "**Option 2 (Hallucinations):**\n",
      "My AI is great, but it keeps making up facts. I asked it for the capital of France, and it said \"Bologna, but only if you're wearing socks.\"\n",
      "\n",
      "**Option 3 (Optimization):**\n",
      "Why did the AI get fired from the bakery?\n",
      "It kept trying to optimize the recipe for maximum flour efficiency, and the cakes tasted like cardboard.\n",
      "\n",
      "**Option 4 (Classic setup):**\n",
      "Why did the AI cross the road?\n",
      "To optimize its pathfinding algorithm. \n",
      " word count - 107\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(temperature=0,model=\"gemini-2.5-flash\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'word_count': RunnableLambda(word_count)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({'topic':'AI'})\n",
    "\n",
    "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aba9df",
   "metadata": {},
   "source": [
    "### What is RunnableBranch?\n",
    "\n",
    "RunnableBranch is a control-flow primitive in LangChain.\n",
    "\n",
    "It works like an if-elif-else statement inside a Runnable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3d71f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Russia-Ukraine conflict, which escalated into a full-scale invasion on February 24, 2022, is the largest conventional war in Europe since World War II. Rooted in centuries of complex historical ties, post-Soviet geopolitical shifts, and Ukraine's aspirations for closer integration with the West, the conflict saw Russia annex Crimea in 2014 and support separatists in Donbas, leading to an eight-year low-intensity war before the 2022 invasion.\n",
      "\n",
      "Russia's initial attempt for a swift victory and regime change in Kyiv failed due to fierce Ukrainian resistance. The conflict evolved into a protracted war of attrition, with Russia shifting focus to Donbas. Ukraine launched successful counter-offensives in late 2022 (Kharkiv, Kherson), but a 2023 offensive has seen slow progress against fortified Russian lines, leading to a current military stalemate.\n",
      "\n",
      "Key players include Ukrainian President Volodymyr Zelenskyy, Russian President Vladimir Putin, and international actors like the United States, NATO, and the European Union, who provide extensive support to Ukraine, while China maintains a partnership with Russia.\n",
      "\n",
      "The conflict has resulted in a profound humanitarian crisis with millions displaced and thousands of casualties, widespread allegations of war crimes, and massive destruction of Ukrainian infrastructure. Economically, it has led to unprecedented sanctions on Russia, global energy and food price spikes, and significant economic uncertainty. Geopolitically, it has reshaped European security, isolated Russia from the West, and raised concerns about international law.\n",
      "\n",
      "With no clear diplomatic resolution in sight, challenges include sustaining Western support, Russia's resilience, and the monumental task of Ukraine's reconstruction. The conflict continues to have profound and enduring consequences for global order.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(temperature=0,model=\"gemini-2.5-flash\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91075a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
